{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d7ad6a0-80f5-4fe8-b680-3a7d2fc19fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAM: 4.2 GB\n"
     ]
    }
   ],
   "source": [
    "from psutil import virtual_memory\n",
    "ram_gb = virtual_memory().total / 1e9\n",
    "print('RAM: {:.1f} GB'.format(ram_gb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "82c22ffb-007b-40bf-b06a-a1b6d7102714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "\n",
    "print(multiprocessing.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "83f52464-3898-408e-8e98-21e9320ebbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "import datetime\n",
    "\n",
    "import netCDF4\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d19ee591-6811-452c-bf6a-94af696d7a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input\n",
    "year_input          = '2024'\n",
    "path_input_folder   = 'D:\\\\CYGNSS_L1_V3.2_3.2-20250317_135601\\\\'\n",
    "\n",
    "# Coordinate VN - WGS84-Zone0\n",
    "lat_start, lat_end = 0, 24\n",
    "lon_start, lon_end = 102, 110 \n",
    "\n",
    "# ouput\n",
    "path_output_folder  = 'D:\\\\raw_vn\\\\20241\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b1afd262-de0b-49ef-aa70-d7e2d721566c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'20250319_20h32'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def Present(print = False):\n",
    "    # get timezone HN (UTC +7)\n",
    "    tz_hanoi = datetime.timezone(datetime.timedelta(hours=7))\n",
    "    # get present time\n",
    "    now     = datetime.datetime.now(tz_hanoi)\n",
    "    # convert to str ...\n",
    "    dt_str  = now.strftime(\"%Y/%m/%d %H:%M\")\n",
    "    dt_str  = dt_str.replace('/', '')\n",
    "    dt_str  = dt_str.replace(':', 'h')\n",
    "    dt_str  = dt_str.replace(' ', '_')\n",
    "    # print???\n",
    "    if print == True:\n",
    "        print (dt_str)\n",
    "    # return\n",
    "    return dt_str\n",
    "Present()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "84a89423-14db-417d-a153-754f71a1bf5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "def FilterByCoordinates (lat_, lon_):\n",
    "    if (lat_ >= lat_start and lat_ <= lat_end) and (lon_ >= lon_start and lon_ <= lon_end):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "print(FilterByCoordinates(19,106))\n",
    "print(FilterByCoordinates(16,107))\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "def FilterPerData (data_):\n",
    "    lats_ = data_.variables['sp_lat'][:]\n",
    "    lons_ = data_.variables['sp_lon'][:]\n",
    "\n",
    "    index_valid_ = []\n",
    "    for i in range (0, len(lats_)):\n",
    "        for j in range (0, 4):\n",
    "            lt_ = lats_[i][j]\n",
    "            ln_ = lons_[i][j]\n",
    "            if (FilterByCoordinates(lt_, ln_) == True):\n",
    "                index_valid_.append(i)\n",
    "                break\n",
    "            else: continue\n",
    "    # data_.close()\n",
    "    return index_valid_\n",
    "#-------------------------------------------------------------------------------\n",
    "def getDictAttr (netCDF4Dataset):\n",
    "    metadata = {}\n",
    "    for attrname in netCDF4Dataset.ncattrs():\n",
    "        metadata[attrname] = netCDF4Dataset.getncattr(attrname)\n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d4f7ce99-3029-4874-9c3c-ccd65a579c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      " 0 cyg01.ddmi.s20240911-000000-e20240911-235959.l1.power-brcs.a32.d33.nc\n",
      "---\n",
      " 1 cyg01.ddmi.s20240912-000000-e20240912-235959.l1.power-brcs.a32.d33.nc\n",
      "---\n",
      " 2 cyg02.ddmi.s20240911-000000-e20240911-235959.l1.power-brcs.a32.d33.nc\n",
      "---\n",
      " 3 cyg02.ddmi.s20240912-000000-e20240912-235959.l1.power-brcs.a32.d33.nc\n",
      "---\n",
      " 4 cyg03.ddmi.s20240911-000000-e20240911-235959.l1.power-brcs.a32.d33.nc\n",
      "---\n",
      " 5 cyg03.ddmi.s20240912-000000-e20240912-235959.l1.power-brcs.a32.d33.nc\n",
      "---\n",
      " 6 cyg04.ddmi.s20240911-000000-e20240911-235959.l1.power-brcs.a32.d33.nc\n",
      "---\n",
      " 7 cyg04.ddmi.s20240912-000000-e20240912-235959.l1.power-brcs.a32.d33.nc\n",
      "---\n",
      " 8 cyg05.ddmi.s20240911-000000-e20240911-235959.l1.power-brcs.a32.d33.nc\n",
      "---\n",
      " 9 cyg05.ddmi.s20240912-000000-e20240912-235959.l1.power-brcs.a32.d33.nc\n",
      "---\n",
      " 10 cyg07.ddmi.s20240911-000000-e20240911-235959.l1.power-brcs.a32.d33.nc\n",
      "---\n",
      " 11 cyg07.ddmi.s20240912-000000-e20240912-235959.l1.power-brcs.a32.d33.nc\n",
      "---\n",
      " 12 cyg08.ddmi.s20240911-000000-e20240911-235959.l1.power-brcs.a32.d33.nc\n",
      "---\n",
      " 13 cyg08.ddmi.s20240912-000000-e20240912-235959.l1.power-brcs.a32.d33.nc\n"
     ]
    }
   ],
   "source": [
    "path_input_files = glob.glob(os.path.join(path_input_folder, \"*.nc\"))\n",
    "\n",
    "check_num = 0\n",
    "\n",
    "for path in path_input_files:\n",
    "    input_filename_p    = path.split('\\\\')[-1]\n",
    "    output_filename_p   = input_filename_p.replace('.nc', '.vn.nc')\n",
    "\n",
    "    # print\n",
    "    print('---\\n', check_num, input_filename_p)\n",
    "\n",
    "    with netCDF4.Dataset(path, moder = 'r') as netCDF4_input_p:\n",
    "        #-----------------------------------------------------------------------\n",
    "        # Get index valid to my AOI - Area Of Interest\n",
    "        index_valid_p   = FilterPerData(netCDF4_input_p)\n",
    "\n",
    "        #-----------------------------------------------------------------------\n",
    "        # get dimensions of dataset\n",
    "        dimensions_p    = {dimname: len(netCDF4_input_p.dimensions[dimname]) for dimname in netCDF4_input_p.dimensions.keys()}\n",
    "\n",
    "        # create dict dimensions valid\n",
    "        dimensions_p['sample']  = len(index_valid_p)\n",
    "        dict_dimensions_valid_p = dimensions_p\n",
    "        list_dimensions_keys_p  = list(dict_dimensions_valid_p.keys())\n",
    "\n",
    "        #-----------------------------------------------------------------------\n",
    "        # Get and create dict global variables of CDF4 dataset\n",
    "        dict_global_variables_p                  = getDictAttr(netCDF4_input_p)\n",
    "        dict_global_variables_p['lat_filter']    = [lat_start, lat_end]\n",
    "        dict_global_variables_p['lon_filter']    = [lon_start, lon_end]\n",
    "\n",
    "\n",
    "        #-----------------------------------------------------------------------\n",
    "        # Get important variables from CDF4 dataset\n",
    "        input_variables_p       = netCDF4_input_p.variables\n",
    "        input_variables_keys_p  = list(input_variables_p.keys())\n",
    "\n",
    "\n",
    "        # filter important variables by index_valid_p\n",
    "        dict_important_variables_valid     = {}\n",
    "        for vkey in input_variables_keys_p: # duyệt theo variables keys của netcdf4 data\n",
    "            # lấy data từ netcdf dataset\n",
    "            variables_keys_p_vkey = input_variables_p[vkey]\n",
    "            try:\n",
    "                # nếu dimension hợp lý so vs index valid thì thêm vào variables mới\n",
    "                dict_important_variables_valid[vkey]    = [\n",
    "                                                            variables_keys_p_vkey.dtype, # cái này là type của variables để tạo variables mới\n",
    "                                                            variables_keys_p_vkey[:][index_valid_p] # cái này là dữ liệu được trích xuất bằng index_valid\n",
    "                                                        ]\n",
    "            except:\n",
    "                # nếu không thì chuyển cái đó thành global variables - metadata\n",
    "                dict_global_variables_p[vkey]           = variables_keys_p_vkey[:]\n",
    "\n",
    "        #-----------------------------------------------------------------------\n",
    "        # export dataset to new netCDF4 file\n",
    "        with netCDF4.Dataset(path_output_folder + output_filename_p, \"w\", format=\"NETCDF4\") as output_file_p:\n",
    "            #------------------------\n",
    "            # create global attribute\n",
    "            output_file_p.setncatts(dict_global_variables_p)\n",
    "\n",
    "            #------------------------\n",
    "            # create dimension\n",
    "            for dim_name, dim_option in dict_dimensions_valid_p.items():\n",
    "                output_file_p.createDimension(\n",
    "                                                dim_name,\n",
    "                                                dim_option\n",
    "                                            )\n",
    "            # display(output_file_p)\n",
    "\n",
    "            #------------------------\n",
    "            # create important variables\n",
    "            for var_name, (var_dtype, var_data) in dict_important_variables_valid.items():\n",
    "                # Lấy thứ tự tên dimensions của biến gốc (nếu có)\n",
    "                orig_dims = input_variables_p[var_name].dimensions  # Đây là tuple chứa tên các dimension của biến gốc\n",
    "                dims_to_use = []\n",
    "                # Với mỗi chiều của var_data, xác định tên dimension sẽ dùng\n",
    "                for i in range(len(var_data.shape)):\n",
    "                    if i < len(orig_dims):\n",
    "                        # Sử dụng tên dimension gốc nếu có\n",
    "                        dim_name = orig_dims[i]\n",
    "                    else:\n",
    "                        # Nếu số chiều của dữ liệu lớn hơn số tên dimensions gốc, tạo tên mới theo thứ tự\n",
    "                        dim_name = \"dim{}\".format(i)\n",
    "                    dims_to_use.append(dim_name)\n",
    " \n",
    "                    # Kiểm tra xem dimension đã tồn tại trong file xuất chưa\n",
    "                    if dim_name not in output_file_p.dimensions:\n",
    "                        # Nếu chưa, tạo dimension với kích thước theo var_data.shape[i]\n",
    "                        output_file_p.createDimension(dim_name, var_data.shape[i])\n",
    "                    else:\n",
    "                        # Nếu đã tồn tại nhưng kích thước khác, tạo dimension mới với tên thay đổi\n",
    "                        if output_file_p.dimensions[dim_name].size != var_data.shape[i]:\n",
    "                            new_dim_name = dim_name + \"_new\"\n",
    "                            dims_to_use[-1] = new_dim_name\n",
    "                            if new_dim_name not in output_file_p.dimensions:\n",
    "                                output_file_p.createDimension(new_dim_name, var_data.shape[i])\n",
    " \n",
    "                # Tạo biến mới với tên, kiểu dữ liệu và dimensions đã xác định\n",
    "                new_var = output_file_p.createVariable(var_name, var_dtype, dimensions=dims_to_use)\n",
    " \n",
    "                # Lấy các thuộc tính của biến gốc và set cho biến mới\n",
    "                dict_attr_var_i = getDictAttr(input_variables_p[var_name])\n",
    "                new_var.setncatts(dict_attr_var_i)\n",
    " \n",
    "                # Gán dữ liệu cho biến mới\n",
    "                new_var[:] = var_data\n",
    "\n",
    "\n",
    "\n",
    "    check_num += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "858adea6-8d94-447e-b0eb-8f06b0a53a7b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      " 1 D:\\HThiem\\cygnss_to_csv\\cyg01.ddmi.s20240911-000000-e20240911-235959.l1.power-brcs.a32.d33.vn.csv\n",
      "---\n",
      " 2 D:\\HThiem\\cygnss_to_csv\\cyg01.ddmi.s20240912-000000-e20240912-235959.l1.power-brcs.a32.d33.vn.csv\n",
      "---\n",
      " 3 D:\\HThiem\\cygnss_to_csv\\cyg02.ddmi.s20240911-000000-e20240911-235959.l1.power-brcs.a32.d33.vn.csv\n",
      "---\n",
      " 4 D:\\HThiem\\cygnss_to_csv\\cyg02.ddmi.s20240912-000000-e20240912-235959.l1.power-brcs.a32.d33.vn.csv\n",
      "---\n",
      " 5 D:\\HThiem\\cygnss_to_csv\\cyg03.ddmi.s20240911-000000-e20240911-235959.l1.power-brcs.a32.d33.vn.csv\n",
      "---\n",
      " 6 D:\\HThiem\\cygnss_to_csv\\cyg03.ddmi.s20240912-000000-e20240912-235959.l1.power-brcs.a32.d33.vn.csv\n",
      "---\n",
      " 7 D:\\HThiem\\cygnss_to_csv\\cyg04.ddmi.s20240911-000000-e20240911-235959.l1.power-brcs.a32.d33.vn.csv\n",
      "---\n",
      " 8 D:\\HThiem\\cygnss_to_csv\\cyg04.ddmi.s20240912-000000-e20240912-235959.l1.power-brcs.a32.d33.vn.csv\n",
      "---\n",
      " 9 D:\\HThiem\\cygnss_to_csv\\cyg05.ddmi.s20240911-000000-e20240911-235959.l1.power-brcs.a32.d33.vn.csv\n",
      "---\n",
      " 10 D:\\HThiem\\cygnss_to_csv\\cyg05.ddmi.s20240912-000000-e20240912-235959.l1.power-brcs.a32.d33.vn.csv\n",
      "---\n",
      " 11 D:\\HThiem\\cygnss_to_csv\\cyg07.ddmi.s20240911-000000-e20240911-235959.l1.power-brcs.a32.d33.vn.csv\n",
      "---\n",
      " 12 D:\\HThiem\\cygnss_to_csv\\cyg07.ddmi.s20240912-000000-e20240912-235959.l1.power-brcs.a32.d33.vn.csv\n",
      "---\n",
      " 13 D:\\HThiem\\cygnss_to_csv\\cyg08.ddmi.s20240911-000000-e20240911-235959.l1.power-brcs.a32.d33.vn.csv\n",
      "---\n",
      " 14 D:\\HThiem\\cygnss_to_csv\\cyg08.ddmi.s20240912-000000-e20240912-235959.l1.power-brcs.a32.d33.vn.csv\n"
     ]
    }
   ],
   "source": [
    "import arrow\n",
    "import glob\n",
    "import netCDF4\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "check_num = 0\n",
    "# Thư mục chứa file NetCDF4\n",
    "folder_path = r\"D:\\raw_vn\\20241\"\n",
    "\n",
    "# Lấy danh sách tất cả các file .nc trong thư mục\n",
    "nc_files = glob.glob(os.path.join(folder_path, \"*.nc\"))\n",
    "\n",
    "pi = 3.14\n",
    "lamda = 0.19\n",
    "# Thư mục lưu file CSV\n",
    "output_folder = r\"D:\\HThiem\\cygnss_to_csv\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Lặp qua từng file và xuất dữ liệu\n",
    "for file in nc_files:\n",
    "    with netCDF4.Dataset(file, mode=\"r\") as nc:\n",
    "\n",
    "        # Lấy danh sách tất cả biến trong file\n",
    "        #variables = list(nc.variables.keys())\n",
    "\n",
    "        # Đọc dữ liệu từ tất cả biến vào DataFrame\n",
    "        #data_dict = {var: nc.variables[var][:].flatten() for var in variables}\n",
    "\n",
    "        # Xác định kích thước nhỏ nhất để đồng bộ dữ liệu\n",
    "        #min_length = min(len(v) for v in data_dict.values())\n",
    "        #for key in data_dict:\n",
    "        #    data_dict[key] = data_dict[key][:min_length]\n",
    "\n",
    "        # Tạo DataFrame từ toàn bộ dữ liệu\n",
    "        #df = pd.DataFrame(data_dict)\n",
    "        sp_lat = nc.variables['sp_lat'][:]\n",
    "        sp_lon = nc.variables['sp_lon'][:]\n",
    "        ddm_snr = nc.variables['ddm_snr'][:]\n",
    "        ddm_nbrcs = nc.variables['ddm_nbrcs'][:]\n",
    "        gps_eirp = nc.variables['gps_eirp'][:]\n",
    "        tx_to_sp_range = nc.variables['tx_to_sp_range'][:]\n",
    "        rx_to_sp_range = nc.variables['rx_to_sp_range'][:]\n",
    "        sp_inc_angle = nc.variables['sp_inc_angle'][:]\n",
    "        sp_rx_gain = nc.variables['sp_rx_gain'][:]\n",
    "        quality_flags = nc.variables['quality_flags'][:]\n",
    "        quality_flags_2 = nc.variables['quality_flags_2'] [:]\n",
    "        gps_tx_power_db_w = nc.variables['gps_tx_power_db_w'][:]\n",
    "        gps_ant_gain_db_i = nc.variables['gps_ant_gain_db_i'][:]\t\n",
    "        ddm_noise_floor = nc.variables['ddm_noise_floor'][:]\n",
    "        ddm_nbrcs_peak = nc.variables['ddm_nbrcs_peak'][:]\n",
    "        \n",
    "        df = pd.DataFrame([])\n",
    "        df['sp_lat'] = sp_lat.flatten()\n",
    "        df['sp_lon'] = sp_lon.flatten()\n",
    "        df['ddm_noise_floor'] = ddm_noise_floor.flatten()\n",
    "        df['ddm_snr'] = ddm_snr.flatten()\n",
    "        df['gps_tx_power_db_w'] = gps_tx_power_db_w.flatten()\n",
    "        df['gps_ant_gain_db_i'] = gps_ant_gain_db_i.flatten()\n",
    "        df['ddm_nbrcs'] = ddm_nbrcs.flatten()\n",
    "        df['gps_eirp'] = gps_eirp.flatten()\n",
    "        df['tx_to_sp_range'] = tx_to_sp_range.flatten()\n",
    "        df['rx_to_sp_range'] = rx_to_sp_range.flatten()\n",
    "        df['sp_inc_angle'] = sp_inc_angle.flatten()\n",
    "        df['sp_rx_gain'] = sp_rx_gain.flatten()\n",
    "        df['quality_flags'] = quality_flags.flatten()\n",
    "        df['quality_flags_2'] = quality_flags_2.flatten()\n",
    "        df['ddm_nbrcs_peak'] = ddm_nbrcs_peak.flatten()\n",
    "        \n",
    "        #df['SR']=10*np.log10(df['ddm_snr']) - 10*np.log10(df['gps_eirp']) - 10*np.log10(sp_rx_gain) + 20*np.log10(df['tx_to_sp_range']+df['rx_to_sp_range']) + 20*np.log10(4*pi) - 20*np.log10(lamda) \n",
    "        #df['SR']= df['SR'] - 10*np.log10(np.cos(np.radian(df['sp_inc_angle'])))\n",
    "        df = df.replace(-9999, float('nan'))\n",
    "        # Xuất ra file CSV\n",
    "        csv_filename = os.path.join(output_folder, os.path.basename(file).replace(\".nc\", \".csv\"))\n",
    "        df.to_csv(csv_filename, index=False)\n",
    "        check_num += 1;\n",
    "        print('---\\n', check_num, csv_filename)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fe0e3868-b5ed-4fa7-9413-e141b174ea50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\pandas\\core\\arraylike.py:399: RuntimeWarning: invalid value encountered in log10\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_9244\\2472069065.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, 'SR']=df['ddm_snr'] - 10*np.log10(df['gps_tx_power_db_w']) - 10*np.log10(df['gps_ant_gain_db_i']) - 10*np.log10(df['sp_rx_gain']) + 20*np.log10(df['tx_to_sp_range']+df['rx_to_sp_range']) + 20*np.log10(4*pi) - 20*np.log10(lamda)\n",
      "C:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\pandas\\core\\arraylike.py:399: RuntimeWarning: invalid value encountered in log10\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_9244\\2472069065.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, 'SR']=df['ddm_snr'] - 10*np.log10(df['gps_tx_power_db_w']) - 10*np.log10(df['gps_ant_gain_db_i']) - 10*np.log10(df['sp_rx_gain']) + 20*np.log10(df['tx_to_sp_range']+df['rx_to_sp_range']) + 20*np.log10(4*pi) - 20*np.log10(lamda)\n",
      "C:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\pandas\\core\\arraylike.py:399: RuntimeWarning: invalid value encountered in log10\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_9244\\2472069065.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, 'SR']=df['ddm_snr'] - 10*np.log10(df['gps_tx_power_db_w']) - 10*np.log10(df['gps_ant_gain_db_i']) - 10*np.log10(df['sp_rx_gain']) + 20*np.log10(df['tx_to_sp_range']+df['rx_to_sp_range']) + 20*np.log10(4*pi) - 20*np.log10(lamda)\n",
      "C:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\pandas\\core\\arraylike.py:399: RuntimeWarning: invalid value encountered in log10\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_9244\\2472069065.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, 'SR']=df['ddm_snr'] - 10*np.log10(df['gps_tx_power_db_w']) - 10*np.log10(df['gps_ant_gain_db_i']) - 10*np.log10(df['sp_rx_gain']) + 20*np.log10(df['tx_to_sp_range']+df['rx_to_sp_range']) + 20*np.log10(4*pi) - 20*np.log10(lamda)\n",
      "C:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\pandas\\core\\arraylike.py:399: RuntimeWarning: invalid value encountered in log10\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_9244\\2472069065.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, 'SR']=df['ddm_snr'] - 10*np.log10(df['gps_tx_power_db_w']) - 10*np.log10(df['gps_ant_gain_db_i']) - 10*np.log10(df['sp_rx_gain']) + 20*np.log10(df['tx_to_sp_range']+df['rx_to_sp_range']) + 20*np.log10(4*pi) - 20*np.log10(lamda)\n",
      "C:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\pandas\\core\\arraylike.py:399: RuntimeWarning: invalid value encountered in log10\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_9244\\2472069065.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, 'SR']=df['ddm_snr'] - 10*np.log10(df['gps_tx_power_db_w']) - 10*np.log10(df['gps_ant_gain_db_i']) - 10*np.log10(df['sp_rx_gain']) + 20*np.log10(df['tx_to_sp_range']+df['rx_to_sp_range']) + 20*np.log10(4*pi) - 20*np.log10(lamda)\n",
      "C:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\pandas\\core\\arraylike.py:399: RuntimeWarning: invalid value encountered in log10\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_9244\\2472069065.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, 'SR']=df['ddm_snr'] - 10*np.log10(df['gps_tx_power_db_w']) - 10*np.log10(df['gps_ant_gain_db_i']) - 10*np.log10(df['sp_rx_gain']) + 20*np.log10(df['tx_to_sp_range']+df['rx_to_sp_range']) + 20*np.log10(4*pi) - 20*np.log10(lamda)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\HThiem\\cygnss_to_csv\\20240911.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\pandas\\core\\arraylike.py:399: RuntimeWarning: invalid value encountered in log10\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_9244\\2472069065.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, 'SR']=df['ddm_snr'] - 10*np.log10(df['gps_tx_power_db_w']) - 10*np.log10(df['gps_ant_gain_db_i']) - 10*np.log10(df['sp_rx_gain']) + 20*np.log10(df['tx_to_sp_range']+df['rx_to_sp_range']) + 20*np.log10(4*pi) - 20*np.log10(lamda)\n",
      "C:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\pandas\\core\\arraylike.py:399: RuntimeWarning: invalid value encountered in log10\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_9244\\2472069065.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, 'SR']=df['ddm_snr'] - 10*np.log10(df['gps_tx_power_db_w']) - 10*np.log10(df['gps_ant_gain_db_i']) - 10*np.log10(df['sp_rx_gain']) + 20*np.log10(df['tx_to_sp_range']+df['rx_to_sp_range']) + 20*np.log10(4*pi) - 20*np.log10(lamda)\n",
      "C:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\pandas\\core\\arraylike.py:399: RuntimeWarning: invalid value encountered in log10\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_9244\\2472069065.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, 'SR']=df['ddm_snr'] - 10*np.log10(df['gps_tx_power_db_w']) - 10*np.log10(df['gps_ant_gain_db_i']) - 10*np.log10(df['sp_rx_gain']) + 20*np.log10(df['tx_to_sp_range']+df['rx_to_sp_range']) + 20*np.log10(4*pi) - 20*np.log10(lamda)\n",
      "C:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\pandas\\core\\arraylike.py:399: RuntimeWarning: invalid value encountered in log10\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_9244\\2472069065.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, 'SR']=df['ddm_snr'] - 10*np.log10(df['gps_tx_power_db_w']) - 10*np.log10(df['gps_ant_gain_db_i']) - 10*np.log10(df['sp_rx_gain']) + 20*np.log10(df['tx_to_sp_range']+df['rx_to_sp_range']) + 20*np.log10(4*pi) - 20*np.log10(lamda)\n",
      "C:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\pandas\\core\\arraylike.py:399: RuntimeWarning: invalid value encountered in log10\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_9244\\2472069065.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, 'SR']=df['ddm_snr'] - 10*np.log10(df['gps_tx_power_db_w']) - 10*np.log10(df['gps_ant_gain_db_i']) - 10*np.log10(df['sp_rx_gain']) + 20*np.log10(df['tx_to_sp_range']+df['rx_to_sp_range']) + 20*np.log10(4*pi) - 20*np.log10(lamda)\n",
      "C:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\pandas\\core\\arraylike.py:399: RuntimeWarning: invalid value encountered in log10\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_9244\\2472069065.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, 'SR']=df['ddm_snr'] - 10*np.log10(df['gps_tx_power_db_w']) - 10*np.log10(df['gps_ant_gain_db_i']) - 10*np.log10(df['sp_rx_gain']) + 20*np.log10(df['tx_to_sp_range']+df['rx_to_sp_range']) + 20*np.log10(4*pi) - 20*np.log10(lamda)\n",
      "C:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\pandas\\core\\arraylike.py:399: RuntimeWarning: invalid value encountered in log10\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_9244\\2472069065.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, 'SR']=df['ddm_snr'] - 10*np.log10(df['gps_tx_power_db_w']) - 10*np.log10(df['gps_ant_gain_db_i']) - 10*np.log10(df['sp_rx_gain']) + 20*np.log10(df['tx_to_sp_range']+df['rx_to_sp_range']) + 20*np.log10(4*pi) - 20*np.log10(lamda)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\HThiem\\cygnss_to_csv\\20240912.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from glob import glob\n",
    "check_num = 0\n",
    "csv_folder = \"D:\\\\HThiem\\\\cygnss_to_csv\\\\\"\n",
    "\n",
    "csv_files = glob(os.path.join(csv_folder, \"*.csv\"))\n",
    "\n",
    "file_groups = {}\n",
    "\n",
    "for file in csv_files:\n",
    "    filename = os.path.basename(file)\n",
    "\n",
    "    match = re.search(r\"(\\d{8})\", filename)\n",
    "    if match:\n",
    "        date = match.group(1)\n",
    "\n",
    "        if date not in file_groups:\n",
    "            file_groups[date] = []\n",
    "        file_groups[date].append(file)\n",
    "\n",
    "for date, files in file_groups.items():\n",
    "    data_frames = []\n",
    "    for file in files:\n",
    "        df_n = pd.read_csv(file)\n",
    "        df = df_n.dropna(subset=['ddm_snr', 'gps_tx_power_db_w', 'gps_ant_gain_db_i', 'sp_rx_gain', 'tx_to_sp_range', 'rx_to_sp_range', 'sp_inc_angle'])\n",
    "        df.loc[:, 'SR']=df['ddm_snr'] - 10*np.log10(df['gps_tx_power_db_w']) - 10*np.log10(df['gps_ant_gain_db_i']) - 10*np.log10(df['sp_rx_gain']) + 20*np.log10(df['tx_to_sp_range']+df['rx_to_sp_range']) + 20*np.log10(4*pi) - 20*np.log10(lamda) \n",
    "        df.loc[:, 'SR']= df['SR'] - 10*np.log10(np.cos(np.radians(df['sp_inc_angle'])))\n",
    "        data_frames.append(df)\n",
    "                \n",
    "    merged_df = pd.concat(data_frames, ignore_index=True)\n",
    "    merged_df = merged_df.dropna(subset=['SR'])\n",
    "    output_file = os.path.join(csv_folder, f\"{date}.csv\")\n",
    "    merged_df.to_csv(output_file, index=False)\n",
    "    check_num += 1\n",
    "    print(output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5cc255",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "\n",
    "# Định nghĩa các hằng số\n",
    "lamda = 0.19\n",
    "pi = 3.14\n",
    "\n",
    "# Thư mục chứa các file CSV đầu vào\n",
    "input_folder = r\"D:\\HThiem\\cygnss_to_csv\"\n",
    "# Thư mục chứa các file CSV đầu ra\n",
    "output_folder = r\"D:\\output_csv_sr\"\n",
    "\n",
    "# Tạo thư mục đầu ra nếu chưa tồn tại\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Lặp qua từng file CSV trong thư mục đầu vào\n",
    "for filename in os.listdir(input_folder):\n",
    "    if filename.endswith(\".csv\"):  # Chỉ xử lý file CSV\n",
    "        file_path = os.path.join(input_folder, filename)\n",
    "\n",
    "        # Đọc dữ liệu CSV\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # Kiểm tra các cột cần thiết có trong dữ liệu không\n",
    "        required_columns = ['ddm_snr', 'gps_eirp', 'sp_rx_gain', \n",
    "                            'tx_to_sp_range', 'rx_to_sp_range', 'sp_inc_angle']\n",
    "        if all(col in df.columns for col in required_columns):\n",
    "            # Tính SR theo công thức\n",
    "            df['SR'] = (10 * np.log10(df['ddm_snr']) \n",
    "                        - 10 * np.log10(df['gps_eirp']) \n",
    "                        - 10 * np.log10(df['sp_rx_gain'])  \n",
    "                        + 20 * np.log10(df['tx_to_sp_range'] + df['rx_to_sp_range'])  \n",
    "                        + 20 * np.log10(4 * pi) \n",
    "                        - 20 * np.log10(lamda))\n",
    "\n",
    "            # Thêm hiệu chỉnh góc tới\n",
    "            df['SR'] = df['SR'] - 10 * np.log10(np.cos(np.radians(df['sp_inc_angle'])))\n",
    "\n",
    "            # Định nghĩa đường dẫn file CSV đầu ra\n",
    "            output_path = os.path.join(output_folder, f\"sr_{filename}\")\n",
    "\n",
    "            # Lưu kết quả ra file CSV mới\n",
    "            df.to_csv(output_path, index=False)\n",
    "            print(f\"Đã xử lý và lưu: {output_path}\")\n",
    "        else:\n",
    "            print(f\"Bỏ qua {filename}: Thiếu một hoặc nhiều cột cần thiết.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8124b33c-8bc5-4289-aa6a-0bdc077de7b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline của file 20240911.csv: 149.1671 dB\n",
      "Đã lưu file hiệu chuẩn: D:\\HThiem\\cygnss_to_csv\\cygnss_to_csv_0\\20240911_calibrated.csv\n",
      "\n",
      "Baseline của file 20240912.csv: 149.3550 dB\n",
      "Đã lưu file hiệu chuẩn: D:\\HThiem\\cygnss_to_csv\\cygnss_to_csv_0\\20240912_calibrated.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def calibrate_sr(sr_values, percentile=5):\n",
    "    sr_array = np.array(sr_values)\n",
    "    # Tính giá trị ngưỡng tương ứng với phần trăm đã cho\n",
    "    threshold = np.percentile(sr_array, percentile)\n",
    "    \n",
    "    # Lấy các giá trị thuộc bottom percentile\n",
    "    bottom_values = sr_array[sr_array <= threshold]\n",
    "    \n",
    "    # Tính baseline là trung bình của các giá trị trong bottom percentile\n",
    "    baseline = np.mean(bottom_values)\n",
    "    \n",
    "    # Hiệu chuẩn SR: trừ baseline khỏi tất cả các giá trị SR\n",
    "    calibrated_sr = sr_array - baseline\n",
    "    \n",
    "    return calibrated_sr, baseline\n",
    "\n",
    "# Đường dẫn tới folder chứa các file CSV\n",
    "folder_path = r\"D:\\HThiem\\cygnss_to_csv\\cygnss_to_csv_0\"  # Thay đổi đường dẫn này cho phù hợp\n",
    "\n",
    "# Lấy danh sách các file CSV trong folder\n",
    "csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "\n",
    "# Duyệt qua từng file CSV\n",
    "for file_name in csv_files:\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    \n",
    "    # Đọc file CSV\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Tính baseline và hiệu chuẩn cho cột SR\n",
    "    sr_values = df['SR'].values\n",
    "    calibrated_sr, baseline = calibrate_sr(sr_values)\n",
    "    \n",
    "    print(f\"Baseline của file {file_name}: {baseline:.4f} dB\")\n",
    "    \n",
    "    # Thêm cột SR hiệu chuẩn vào DataFrame\n",
    "    df.loc[:, 'SR_0'] = calibrated_sr\n",
    "    \n",
    "    # Lưu file CSV đã hiệu chuẩn, thêm hậu tố '_calibrated' vào tên file\n",
    "    output_file = os.path.join(folder_path, file_name.replace(\".csv\", \"_calibrated.csv\"))\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"Đã lưu file hiệu chuẩn: {output_file}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1d8a63de-da99-47ff-85c8-50b18d4b4267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      " 1 D:\\output_csv_test\\20240102_merged_filtered.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "check_num = 0\n",
    "csv_folder = \"D:\\\\output_csv_test\\\\\"\n",
    "\n",
    "csv_files = glob(os.path.join(csv_folder, \"*.csv\"))\n",
    "\n",
    "for file in csv_files:\n",
    "    df = pd.read_csv(file)\n",
    "\n",
    "    # Lọc dữ liệu theo điều kiện\n",
    "    df_filtered = df[\n",
    "        (df[\"sp_lat\"] > 8.0) &\n",
    "        (df[\"sp_lat\"] < 24.0) &\n",
    "        (df[\"sp_lon\"] > 102.0) &\n",
    "        (df[\"sp_lon\"] < 110.0) &\n",
    "        (df[\"sp_inc_angle\"] < 65.0) &\n",
    "        (df[\"ddm_snr\"] > 1.5) &\n",
    "        (df[\"sp_rx_gain\"] > 0.0)\n",
    "    ]\n",
    "\n",
    "    filter_file = file.replace(\".csv\", \"_filtered.csv\")\n",
    "    df_filtered.to_csv(filter_file, index=False)\n",
    "    check_num += 1\n",
    "    print('---\\n', check_num, filter_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1307aebc-85ec-4133-954c-cebfd3bb659a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      " 1 D:\\HThiem\\cygnss_to_csv\\cygnss_to_csv_0\\20240911_shp.csv\n",
      "---\n",
      " 2 D:\\HThiem\\cygnss_to_csv\\cygnss_to_csv_0\\20240911_calibrated_shp.csv\n",
      "---\n",
      " 3 D:\\HThiem\\cygnss_to_csv\\cygnss_to_csv_0\\20240912_shp.csv\n",
      "---\n",
      " 4 D:\\HThiem\\cygnss_to_csv\\cygnss_to_csv_0\\20240912_calibrated_shp.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "\n",
    "check_num = 0\n",
    "\n",
    "shp_path = r\"D:\\New folder\\shp\\vnm_admbnda_adm0_gov_20200103.shp\"\n",
    "\n",
    "input_csv_folder = r\"D:\\HThiem\\cygnss_to_csv\\cygnss_to_csv_0\"\n",
    "\n",
    "output_folder = r\"D:\\HThiem\\cygnss_to_csv\\cygnss_to_csv_0\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "vn_gdf = gpd.read_file(shp_path)\n",
    "\n",
    "csv_files = glob.glob(os.path.join(input_csv_folder, \"*.csv\"))\n",
    "\n",
    "lat_col = \"sp_lat\"\n",
    "lon_col = \"sp_lon\"\n",
    "\n",
    "for csv_path in csv_files:\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    gdf_points = gpd.GeoDataFrame(\n",
    "        df.copy(),\n",
    "        geometry=gpd.points_from_xy(df[lon_col], df[lat_col]),\n",
    "        crs=vn_gdf.crs\n",
    "    )\n",
    "    \n",
    "    gdf_clip = gpd.clip(gdf_points, vn_gdf)\n",
    "\n",
    "    gdf_clip = gdf_clip.copy()\n",
    "\n",
    "    gdf_clip[\"lon_clip\"] = gdf_clip.geometry.x\n",
    "    gdf_clip[\"lat_clip\"] = gdf_clip.geometry.y\n",
    "\n",
    "    gdf_clip.drop(columns=[\"lon_clip\"], inplace=True)\n",
    "    gdf_clip.drop(columns=[\"lat_clip\"], inplace=True)\n",
    "    gdf_clip.drop(columns=[\"geometry\"], inplace=True)\n",
    "\n",
    "    base_name = os.path.basename(csv_path).replace(\".csv\", \"_shp.csv\")\n",
    "    output_csv = os.path.join(output_folder, base_name)\n",
    "\n",
    "    # Ghi ra file CSV\n",
    "    gdf_clip.to_csv(output_csv, index=False)\n",
    "    check_num += 1\n",
    "    print('---\\n', check_num, output_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d5f1043a-aadb-4514-896c-9bde943db3e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 20240831_calibrated_shp.csv and saved filtered data to D:\\out_csv_vn\\20240831_calibrated_shp_filtered.csv\n",
      "Processed 20240901_calibrated_shp.csv and saved filtered data to D:\\out_csv_vn\\20240901_calibrated_shp_filtered.csv\n",
      "Processed 20240902_calibrated_shp.csv and saved filtered data to D:\\out_csv_vn\\20240902_calibrated_shp_filtered.csv\n",
      "Processed 20240903_calibrated_shp.csv and saved filtered data to D:\\out_csv_vn\\20240903_calibrated_shp_filtered.csv\n",
      "Processed 20240904_calibrated_shp.csv and saved filtered data to D:\\out_csv_vn\\20240904_calibrated_shp_filtered.csv\n",
      "Processed 20240905_calibrated_shp.csv and saved filtered data to D:\\out_csv_vn\\20240905_calibrated_shp_filtered.csv\n",
      "Processed 20240906_calibrated_shp.csv and saved filtered data to D:\\out_csv_vn\\20240906_calibrated_shp_filtered.csv\n",
      "Processed 20240907_calibrated_shp.csv and saved filtered data to D:\\out_csv_vn\\20240907_calibrated_shp_filtered.csv\n",
      "Processed 20240908_calibrated_shp.csv and saved filtered data to D:\\out_csv_vn\\20240908_calibrated_shp_filtered.csv\n",
      "Processed 20240909_calibrated_shp.csv and saved filtered data to D:\\out_csv_vn\\20240909_calibrated_shp_filtered.csv\n",
      "Processed 20240910_calibrated_shp.csv and saved filtered data to D:\\out_csv_vn\\20240910_calibrated_shp_filtered.csv\n",
      "Processed 20240911_calibrated_shp.csv and saved filtered data to D:\\out_csv_vn\\20240911_calibrated_shp_filtered.csv\n",
      "Processed 20240912_calibrated_shp.csv and saved filtered data to D:\\out_csv_vn\\20240912_calibrated_shp_filtered.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define bit masks for the conditions\n",
    "S_BAND_POWERED_UP_MASK = 0x00000002\n",
    "large_sc_attitude_err_MASK = 0x00000008\n",
    "BLACKBODY_DDM_MASK = 0x00000010\n",
    "DDM_IS_TEST_PATTERN_MASK = 0x00000080\n",
    "incorrect_ddmi_antenna_selection_maskk = 0x00000001\n",
    "noise_floor_cal_error_mask = 0x00000004\n",
    "\n",
    "def filter_data(df):\n",
    "    \"\"\"\n",
    "    Filters a DataFrame based on the quality_flags column.\n",
    "    Returns rows where any of the specified bit flags is set.\n",
    "    \"\"\"\n",
    "    filtered = df[\n",
    "        (df['quality_flags'] & S_BAND_POWERED_UP_MASK == 0) |\n",
    "        (df['quality_flags'] & SMALL_SC_ATTITUDE_ERR_MASK != 0) |\n",
    "        (df['quality_flags'] & LARGE_SC_ATTITUDE_ERR_MASK != 0) |\n",
    "        (df['quality_flags'] & DDM_IS_TEST_PATTERN_MASK != 0)|\n",
    "        (df['quality_flags_2'] & incorrect_ddmi_antenna_selection_maskk != 0)|\n",
    "        (df['quality_flags_2'] & noise_floor_cal_error_mask != 0)\n",
    "    ]\n",
    "    return filtered\n",
    "\n",
    "def process_csv_files_in_folder(folder_path, output_folder):\n",
    "    \"\"\"\n",
    "    Processes all CSV files in the given folder. For each CSV file,\n",
    "    filters the data based on quality_flags and writes the result to\n",
    "    a new CSV file with a modified name (originalname_filtered.csv) in the output folder.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.lower().endswith('.csv'):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            try:\n",
    "                df = pd.read_csv(file_path)\n",
    "                # Ensure that quality_flags column is an integer type\n",
    "                df['quality_flags'] = df['quality_flags'].astype(int)\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {filename}: {e}\")\n",
    "                continue\n",
    "\n",
    "            filtered_df = filter_data(df)\n",
    "            output_filename = os.path.splitext(filename)[0] + '_filtered.csv'\n",
    "            output_path = os.path.join(output_folder, output_filename)\n",
    "            try:\n",
    "                filtered_df.to_csv(output_path, index=False)\n",
    "                print(f\"Processed {filename} and saved filtered data to {output_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error writing {output_filename}: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Replace these paths with the actual folder paths on your system\n",
    "    input_folder_path = r'D:\\csv_vn_0'\n",
    "    output_folder_path = r'D:\\out_csv_vn'\n",
    "    \n",
    "    process_csv_files_in_folder(input_folder_path, output_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b28099fd-5d9d-41d9-8fd2-3fda19b1e570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đang đọc dữ liệu CSV...\n",
      "Đang đọc shapefile lưới...\n",
      "Đang chuyển đổi hệ tọa độ...\n",
      "Đang thực hiện nội suy cho file D:\\output\\20240831_calibrated_shp_filtered.csv...\n",
      "Đang xuất kết quả cho file D:\\output\\20240831_calibrated_shp_filtered.csv...\n",
      "Đã xuất file CSV: D:\\output\\noisuy1\\20240831_calibrated_shp_filtered_SR_0_interpolated.csv\n",
      "Đã xuất file GeoTIFF: D:\\output\\noisuy1\\20240831_calibrated_shp_filtered_SR_0_interpolated.tif\n",
      "Đang thực hiện nội suy cho file D:\\output\\20240901_calibrated_shp_filtered.csv...\n",
      "Đang xuất kết quả cho file D:\\output\\20240901_calibrated_shp_filtered.csv...\n",
      "Đã xuất file CSV: D:\\output\\noisuy1\\20240901_calibrated_shp_filtered_SR_0_interpolated.csv\n",
      "Đã xuất file GeoTIFF: D:\\output\\noisuy1\\20240901_calibrated_shp_filtered_SR_0_interpolated.tif\n",
      "Đang thực hiện nội suy cho file D:\\output\\20240902_calibrated_shp_filtered.csv...\n",
      "Đang xuất kết quả cho file D:\\output\\20240902_calibrated_shp_filtered.csv...\n",
      "Đã xuất file CSV: D:\\output\\noisuy1\\20240902_calibrated_shp_filtered_SR_0_interpolated.csv\n",
      "Đã xuất file GeoTIFF: D:\\output\\noisuy1\\20240902_calibrated_shp_filtered_SR_0_interpolated.tif\n",
      "Đang thực hiện nội suy cho file D:\\output\\20240903_calibrated_shp_filtered.csv...\n",
      "Đang xuất kết quả cho file D:\\output\\20240903_calibrated_shp_filtered.csv...\n",
      "Đã xuất file CSV: D:\\output\\noisuy1\\20240903_calibrated_shp_filtered_SR_0_interpolated.csv\n",
      "Đã xuất file GeoTIFF: D:\\output\\noisuy1\\20240903_calibrated_shp_filtered_SR_0_interpolated.tif\n",
      "Đang thực hiện nội suy cho file D:\\output\\20240904_calibrated_shp_filtered.csv...\n",
      "Đang xuất kết quả cho file D:\\output\\20240904_calibrated_shp_filtered.csv...\n",
      "Đã xuất file CSV: D:\\output\\noisuy1\\20240904_calibrated_shp_filtered_SR_0_interpolated.csv\n",
      "Đã xuất file GeoTIFF: D:\\output\\noisuy1\\20240904_calibrated_shp_filtered_SR_0_interpolated.tif\n",
      "Đang thực hiện nội suy cho file D:\\output\\20240905_calibrated_shp_filtered.csv...\n",
      "Đang xuất kết quả cho file D:\\output\\20240905_calibrated_shp_filtered.csv...\n",
      "Đã xuất file CSV: D:\\output\\noisuy1\\20240905_calibrated_shp_filtered_SR_0_interpolated.csv\n",
      "Đã xuất file GeoTIFF: D:\\output\\noisuy1\\20240905_calibrated_shp_filtered_SR_0_interpolated.tif\n",
      "Đang thực hiện nội suy cho file D:\\output\\20240906_calibrated_shp_filtered.csv...\n",
      "Đang xuất kết quả cho file D:\\output\\20240906_calibrated_shp_filtered.csv...\n",
      "Đã xuất file CSV: D:\\output\\noisuy1\\20240906_calibrated_shp_filtered_SR_0_interpolated.csv\n",
      "Đã xuất file GeoTIFF: D:\\output\\noisuy1\\20240906_calibrated_shp_filtered_SR_0_interpolated.tif\n",
      "Đang thực hiện nội suy cho file D:\\output\\20240907_calibrated_shp_filtered.csv...\n",
      "Đang xuất kết quả cho file D:\\output\\20240907_calibrated_shp_filtered.csv...\n",
      "Đã xuất file CSV: D:\\output\\noisuy1\\20240907_calibrated_shp_filtered_SR_0_interpolated.csv\n",
      "Đã xuất file GeoTIFF: D:\\output\\noisuy1\\20240907_calibrated_shp_filtered_SR_0_interpolated.tif\n",
      "Đang thực hiện nội suy cho file D:\\output\\20240908_calibrated_shp_filtered.csv...\n",
      "Đang xuất kết quả cho file D:\\output\\20240908_calibrated_shp_filtered.csv...\n",
      "Đã xuất file CSV: D:\\output\\noisuy1\\20240908_calibrated_shp_filtered_SR_0_interpolated.csv\n",
      "Đã xuất file GeoTIFF: D:\\output\\noisuy1\\20240908_calibrated_shp_filtered_SR_0_interpolated.tif\n",
      "Đang thực hiện nội suy cho file D:\\output\\20240909_calibrated_shp_filtered.csv...\n",
      "Đang xuất kết quả cho file D:\\output\\20240909_calibrated_shp_filtered.csv...\n",
      "Đã xuất file CSV: D:\\output\\noisuy1\\20240909_calibrated_shp_filtered_SR_0_interpolated.csv\n",
      "Đã xuất file GeoTIFF: D:\\output\\noisuy1\\20240909_calibrated_shp_filtered_SR_0_interpolated.tif\n",
      "Đang thực hiện nội suy cho file D:\\output\\20240910_calibrated_shp_filtered.csv...\n",
      "Đang xuất kết quả cho file D:\\output\\20240910_calibrated_shp_filtered.csv...\n",
      "Đã xuất file CSV: D:\\output\\noisuy1\\20240910_calibrated_shp_filtered_SR_0_interpolated.csv\n",
      "Đã xuất file GeoTIFF: D:\\output\\noisuy1\\20240910_calibrated_shp_filtered_SR_0_interpolated.tif\n",
      "Đang thực hiện nội suy cho file D:\\output\\20240911_calibrated_shp_filtered.csv...\n",
      "Đang xuất kết quả cho file D:\\output\\20240911_calibrated_shp_filtered.csv...\n",
      "Đã xuất file CSV: D:\\output\\noisuy1\\20240911_calibrated_shp_filtered_SR_0_interpolated.csv\n",
      "Đã xuất file GeoTIFF: D:\\output\\noisuy1\\20240911_calibrated_shp_filtered_SR_0_interpolated.tif\n",
      "Đang thực hiện nội suy cho file D:\\output\\20240912_calibrated_shp_filtered.csv...\n",
      "Đang xuất kết quả cho file D:\\output\\20240912_calibrated_shp_filtered.csv...\n",
      "Đã xuất file CSV: D:\\output\\noisuy1\\20240912_calibrated_shp_filtered_SR_0_interpolated.csv\n",
      "Đã xuất file GeoTIFF: D:\\output\\noisuy1\\20240912_calibrated_shp_filtered_SR_0_interpolated.tif\n",
      "Hoàn thành!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from rasterio.transform import from_origin\n",
    "import rasterio\n",
    "from rasterio.features import rasterize\n",
    "from scipy.interpolate import Rbf\n",
    "\n",
    "def load_csv_data(csv_folder):\n",
    "    \"\"\"\n",
    "    Đọc tất cả các file CSV trong thư mục.\n",
    "    Giả định file CSV có các cột: sp_lat, sp_lon, SR_0\n",
    "    \"\"\"\n",
    "    csv_files = glob.glob(os.path.join(csv_folder, '*.csv'))\n",
    "    # Đọc các file CSV và trả về list các DataFrame\n",
    "    dataframes = [pd.read_csv(csv_file) for csv_file in csv_files]\n",
    "    return dataframes, csv_files\n",
    "\n",
    "def create_points_gdf(df, crs):\n",
    "    \"\"\"\n",
    "    Tạo GeoDataFrame từ DataFrame với các cột sp_lon và sp_lat\n",
    "    \"\"\"\n",
    "    gdf = gpd.GeoDataFrame(\n",
    "        df, \n",
    "        geometry=gpd.points_from_xy(df.sp_lon, df.sp_lat),\n",
    "        crs=\"EPSG:4326\"\n",
    "    )\n",
    "    return gdf.to_crs(crs)\n",
    "\n",
    "def rbf_interpolation(points, values, grid_points, epsilon=1):\n",
    "    \"\"\"\n",
    "    Thực hiện nội suy RBF\n",
    "    points: các điểm quan sát (numpy array với shape (n, 2))\n",
    "    values: giá trị SR_0 tương ứng với các điểm quan sát (numpy array với shape (n,))\n",
    "    grid_points: các điểm cần nội suy (numpy array với shape (m, 2))\n",
    "    epsilon: tham số của hàm RBF\n",
    "    \"\"\"\n",
    "    rbf = Rbf(points[:, 0], points[:, 1], values, function='multiquadric', epsilon=epsilon)\n",
    "    interpolated_values = rbf(grid_points[:, 0], grid_points[:, 1])\n",
    "    return interpolated_values\n",
    "\n",
    "def interpolate_grid(grid, points_gdf, epsilon=1):\n",
    "    \"\"\"\n",
    "    Nội suy cho các ô lưới không có dữ liệu\n",
    "    \"\"\"\n",
    "    # Lấy các điểm và giá trị quan sát\n",
    "    observed_points = np.array([(point.x, point.y) for point in points_gdf.geometry])\n",
    "    observed_values = points_gdf['SR_0'].values\n",
    "    \n",
    "    # Lấy các điểm centroid của ô lưới\n",
    "    grid_points = np.array([(point.x, point.y) for point in grid.geometry.centroid])\n",
    "    \n",
    "    # Nội suy RBF\n",
    "    interpolated_values = rbf_interpolation(observed_points, observed_values, grid_points, epsilon)\n",
    "    \n",
    "    # Gán giá trị nội suy vào grid\n",
    "    grid['SR_0'] = interpolated_values\n",
    "    \n",
    "    return grid\n",
    "\n",
    "def export_outputs(grid, output_folder, output_prefix, crs):\n",
    "    \"\"\"\n",
    "    Xuất kết quả ra file CSV và GeoTIFF\n",
    "    \"\"\"\n",
    "    # Tạo thư mục output nếu chưa tồn tại\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    # Chuyển đổi hệ tọa độ của centroid về EPSG:4326\n",
    "    grid_centroids = grid.geometry.centroid.to_crs(epsg=4326)\n",
    "    \n",
    "    # Xuất CSV với tọa độ centroid (lat/long) và giá trị SR_0\n",
    "    output_csv = os.path.join(output_folder, f'{output_prefix}_SR_0_interpolated.csv')\n",
    "    csv_data = pd.DataFrame({\n",
    "        'x': grid_centroids.x,\n",
    "        'y': grid_centroids.y,\n",
    "        'SR_0': grid['SR_0']\n",
    "    })\n",
    "    csv_data.to_csv(output_csv, index=False)\n",
    "    print(f\"Đã xuất file CSV: {output_csv}\")\n",
    "    \n",
    "    # Xuất GeoTIFF\n",
    "    output_tif = os.path.join(output_folder, f'{output_prefix}_SR_0_interpolated.tif')\n",
    "    \n",
    "    # Tính các thông số cho raster\n",
    "    bounds = grid.total_bounds\n",
    "    res = 3000  # độ phân giải 3km\n",
    "    width = int((bounds[2] - bounds[0]) / res)\n",
    "    height = int((bounds[3] - bounds[1]) / res)\n",
    "    transform = from_origin(bounds[0], bounds[3], res, res)\n",
    "    \n",
    "    # Chuyển từ vector sang raster\n",
    "    shapes = ((geom, value) for geom, value in zip(grid.geometry, grid['SR_0']))\n",
    "    raster = rasterize(\n",
    "        shapes=shapes,\n",
    "        out_shape=(height, width),\n",
    "        transform=transform,\n",
    "        fill=np.nan,\n",
    "        dtype='float32'\n",
    "    )\n",
    "    \n",
    "    # Ghi file GeoTIFF\n",
    "    with rasterio.open(\n",
    "        output_tif,\n",
    "        'w',\n",
    "        driver='GTiff',\n",
    "        height=height,\n",
    "        width=width,\n",
    "        count=1,\n",
    "        dtype=raster.dtype,\n",
    "        crs=crs,\n",
    "        transform=transform\n",
    "    ) as dst:\n",
    "        dst.write(raster, 1)\n",
    "    print(f\"Đã xuất file GeoTIFF: {output_tif}\")\n",
    "\n",
    "def main():\n",
    "    # Đường dẫn input/output\n",
    "    csv_folder = r\"D:\\output\"\n",
    "    grid_shp = r\"D:\\out_csv_vn\\Grid_3x3.shp\"\n",
    "    output_folder = r\"D:\\output\\noisuy1\"\n",
    "    \n",
    "    # Đọc dữ liệu CSV\n",
    "    print(\"Đang đọc dữ liệu CSV...\")\n",
    "    dataframes, csv_files = load_csv_data(csv_folder)\n",
    "    \n",
    "    # Đọc shapefile lưới\n",
    "    print(\"Đang đọc shapefile lưới...\")\n",
    "    grid = gpd.read_file(grid_shp)\n",
    "    \n",
    "    # Chuyển đổi hệ tọa độ sang EPSG:32648 (hoặc hệ tọa độ phù hợp khác)\n",
    "    print(\"Đang chuyển đổi hệ tọa độ...\")\n",
    "    grid = grid.to_crs(\"EPSG:32648\")\n",
    "    \n",
    "    # Loop qua từng DataFrame (tương ứng với từng file CSV)\n",
    "    for df, csv_file in zip(dataframes, csv_files):\n",
    "        # Tạo GeoDataFrame từ dữ liệu CSV\n",
    "        points_gdf = create_points_gdf(df, grid.crs)\n",
    "        \n",
    "        # Nội suy cho các ô lưới\n",
    "        print(f\"Đang thực hiện nội suy cho file {csv_file}...\")\n",
    "        grid_interpolated = interpolate_grid(grid.copy(), points_gdf)\n",
    "        \n",
    "        # Xuất kết quả\n",
    "        output_prefix = os.path.splitext(os.path.basename(csv_file))[0]\n",
    "        print(f\"Đang xuất kết quả cho file {csv_file}...\")\n",
    "        export_outputs(grid_interpolated, output_folder, output_prefix, grid.crs)\n",
    "    \n",
    "    print(\"Hoàn thành!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41e79b62-418e-4595-94e2-c613ffa840ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scipy in c:\\users\\dell\\anaconda3\\lib\\site-packages (1.13.1)\n",
      "Requirement already satisfied: numpy<2.3,>=1.22.4 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from scipy) (1.26.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1057dd5d-be0f-43d4-b060-eb4e386435af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đang đọc dữ liệu CSV...\n",
      "Đang gộp dữ liệu...\n",
      "Đang đọc shapefile lưới...\n",
      "Đang chuyển đổi hệ tọa độ...\n",
      "Đang thực hiện nội suy cho nhóm dữ liệu 1...\n",
      "Đang xuất kết quả cho nhóm dữ liệu 1...\n",
      "Đã xuất file CSV: D:\\output\\noisuy\\group_1_SR_0_interpolated.csv\n",
      "Đã xuất file GeoTIFF: D:\\output\\noisuy\\group_1_SR_0_interpolated.tif\n",
      "Đang thực hiện nội suy cho nhóm dữ liệu 2...\n",
      "Đang xuất kết quả cho nhóm dữ liệu 2...\n",
      "Đã xuất file CSV: D:\\output\\noisuy\\group_2_SR_0_interpolated.csv\n",
      "Đã xuất file GeoTIFF: D:\\output\\noisuy\\group_2_SR_0_interpolated.tif\n",
      "Đang thực hiện nội suy cho nhóm dữ liệu 3...\n",
      "Đang xuất kết quả cho nhóm dữ liệu 3...\n",
      "Đã xuất file CSV: D:\\output\\noisuy\\group_3_SR_0_interpolated.csv\n",
      "Đã xuất file GeoTIFF: D:\\output\\noisuy\\group_3_SR_0_interpolated.tif\n",
      "Đang thực hiện nội suy cho nhóm dữ liệu 4...\n",
      "Đang xuất kết quả cho nhóm dữ liệu 4...\n",
      "Đã xuất file CSV: D:\\output\\noisuy\\group_4_SR_0_interpolated.csv\n",
      "Đã xuất file GeoTIFF: D:\\output\\noisuy\\group_4_SR_0_interpolated.tif\n",
      "Hoàn thành!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from rasterio.transform import from_origin\n",
    "import rasterio\n",
    "from rasterio.features import rasterize\n",
    "from scipy.interpolate import Rbf\n",
    "from datetime import datetime\n",
    "\n",
    "def load_csv_data(csv_folder):\n",
    "    \"\"\"\n",
    "    Đọc tất cả các file CSV trong thư mục và trích xuất ngày từ tên file.\n",
    "    Giả định file CSV có các cột: sp_lat, sp_lon, SR_0\n",
    "    \"\"\"\n",
    "    csv_files = glob.glob(os.path.join(csv_folder, '*.csv'))\n",
    "    dataframes = []\n",
    "    for csv_file in csv_files:\n",
    "        df = pd.read_csv(csv_file)\n",
    "        # Trích xuất ngày từ tên file\n",
    "        date_str = os.path.basename(csv_file).split('_')[0]\n",
    "        df['date'] = datetime.strptime(date_str, '%Y%m%d')\n",
    "        dataframes.append(df)\n",
    "    return dataframes\n",
    "\n",
    "def merge_dataframes(dataframes):\n",
    "    \"\"\"\n",
    "    Gộp các DataFrame lại với nhau và nhóm dữ liệu theo ngày\n",
    "    \"\"\"\n",
    "    df = pd.concat(dataframes)\n",
    "    df = df.sort_values(by='date')\n",
    "    \n",
    "    # Gộp dữ liệu cứ 3 ngày liên tiếp thành 1 nhóm\n",
    "    df['group'] = (df['date'].dt.floor('d').sub(df['date'].dt.floor('d').min()).dt.days // 3).astype(int)\n",
    "    grouped_dfs = [group for _, group in df.groupby('group')]\n",
    "    \n",
    "    return grouped_dfs\n",
    "\n",
    "def create_points_gdf(df, crs):\n",
    "    \"\"\"\n",
    "    Tạo GeoDataFrame từ DataFrame với các cột sp_lon và sp_lat\n",
    "    \"\"\"\n",
    "    gdf = gpd.GeoDataFrame(\n",
    "        df, \n",
    "        geometry=gpd.points_from_xy(df.sp_lon, df.sp_lat),\n",
    "        crs=\"EPSG:4326\"\n",
    "    )\n",
    "    return gdf.to_crs(crs)\n",
    "\n",
    "def rbf_interpolation(points, values, grid_points, epsilon=1):\n",
    "    \"\"\"\n",
    "    Thực hiện nội suy RBF\n",
    "    points: các điểm quan sát (numpy array với shape (n, 2))\n",
    "    values: giá trị SR_0 tương ứng với các điểm quan sát (numpy array với shape (n,))\n",
    "    grid_points: các điểm cần nội suy (numpy array với shape (m, 2))\n",
    "    epsilon: tham số của hàm RBF\n",
    "    \"\"\"\n",
    "    rbf = Rbf(points[:, 0], points[:, 1], values, function='multiquadric', epsilon=epsilon)\n",
    "    interpolated_values = rbf(grid_points[:, 0], grid_points[:, 1])\n",
    "    return interpolated_values\n",
    "\n",
    "def interpolate_grid(grid, points_gdf, epsilon=1):\n",
    "    \"\"\"\n",
    "    Nội suy cho các ô lưới không có dữ liệu\n",
    "    \"\"\"\n",
    "    # Lấy các điểm và giá trị quan sát\n",
    "    observed_points = np.array([(point.x, point.y) for point in points_gdf.geometry])\n",
    "    observed_values = points_gdf['SR_0'].values\n",
    "    \n",
    "    # Lấy các điểm centroid của ô lưới\n",
    "    grid_points = np.array([(point.x, point.y) for point in grid.geometry.centroid])\n",
    "    \n",
    "    # Nội suy RBF\n",
    "    interpolated_values = rbf_interpolation(observed_points, observed_values, grid_points, epsilon)\n",
    "    \n",
    "    # Gán giá trị nội suy vào grid\n",
    "    grid['SR_0'] = interpolated_values\n",
    "    \n",
    "    return grid\n",
    "\n",
    "def export_outputs(grid, output_folder, output_prefix, crs):\n",
    "    \"\"\"\n",
    "    Xuất kết quả ra file CSV và GeoTIFF\n",
    "    \"\"\"\n",
    "    # Tạo thư mục output nếu chưa tồn tại\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    # Chuyển đổi tọa độ centroid sang hệ tọa độ WGS84\n",
    "    grid_centroids = grid.geometry.centroid.to_crs(epsg=4326)\n",
    "    \n",
    "    # Xuất CSV với tọa độ lat, lon và giá trị SR_0\n",
    "    output_csv = os.path.join(output_folder, f'{output_prefix}_SR_0_interpolated.csv')\n",
    "    csv_data = pd.DataFrame({\n",
    "        'lat': grid_centroids.y,\n",
    "        'lon': grid_centroids.x,\n",
    "        'SR_0': grid['SR_0']\n",
    "    })\n",
    "    csv_data.to_csv(output_csv, index=False)\n",
    "    print(f\"Đã xuất file CSV: {output_csv}\")\n",
    "    \n",
    "    # Xuất GeoTIFF\n",
    "    output_tif = os.path.join(output_folder, f'{output_prefix}_SR_0_interpolated.tif')\n",
    "    \n",
    "    # Tính các thông số cho raster\n",
    "    bounds = grid.total_bounds\n",
    "    res = 3000  # độ phân giải 3km\n",
    "    width = int((bounds[2] - bounds[0]) / res)\n",
    "    height = int((bounds[3] - bounds[1]) / res)\n",
    "    transform = from_origin(bounds[0], bounds[3], res, res)\n",
    "    \n",
    "    # Chuyển từ vector sang raster\n",
    "    shapes = ((geom, value) for geom, value in zip(grid.geometry, grid['SR_0']))\n",
    "    raster = rasterize(\n",
    "        shapes=shapes,\n",
    "        out_shape=(height, width),\n",
    "        transform=transform,\n",
    "        fill=np.nan,\n",
    "        dtype='float32'\n",
    "    )\n",
    "    \n",
    "    # Ghi file GeoTIFF\n",
    "    with rasterio.open(\n",
    "        output_tif,\n",
    "        'w',\n",
    "        driver='GTiff',\n",
    "        height=height,\n",
    "        width=width,\n",
    "        count=1,\n",
    "        dtype=raster.dtype,\n",
    "        crs=crs,\n",
    "        transform=transform\n",
    "    ) as dst:\n",
    "        dst.write(raster, 1)\n",
    "    print(f\"Đã xuất file GeoTIFF: {output_tif}\")\n",
    "\n",
    "def main():\n",
    "    # Đường dẫn input/output\n",
    "    csv_folder = r\"D:\\output\"\n",
    "    grid_shp = r\"D:\\out_csv_vn\\Grid_3x3.shp\"\n",
    "    output_folder = r\"D:\\output\\noisuy\"\n",
    "    \n",
    "    # Đọc dữ liệu CSV\n",
    "    print(\"Đang đọc dữ liệu CSV...\")\n",
    "    dataframes = load_csv_data(csv_folder)\n",
    "    \n",
    "    # Gộp dữ liệu cứ 3 ngày liên tiếp thành 1 nhóm\n",
    "    print(\"Đang gộp dữ liệu...\")\n",
    "    grouped_dataframes = merge_dataframes(dataframes)\n",
    "    \n",
    "    # Đọc shapefile lưới\n",
    "    print(\"Đang đọc shapefile lưới...\")\n",
    "    grid = gpd.read_file(grid_shp)\n",
    "    \n",
    "    # Chuyển đổi hệ tọa độ sang EPSG:32648 (hoặc hệ tọa độ phù hợp khác)\n",
    "    print(\"Đang chuyển đổi hệ tọa độ...\")\n",
    "    grid = grid.to_crs(\"EPSG:32648\")\n",
    "    \n",
    "    # Loop qua từng nhóm DataFrame (kết quả của việc gộp dữ liệu)\n",
    "    for i, df in enumerate(grouped_dataframes):\n",
    "        # Tạo GeoDataFrame từ dữ liệu CSV\n",
    "        points_gdf = create_points_gdf(df, grid.crs)\n",
    "        \n",
    "        # Nội suy cho các ô lưới\n",
    "        print(f\"Đang thực hiện nội suy cho nhóm dữ liệu {i+1}...\")\n",
    "        grid_interpolated = interpolate_grid(grid.copy(), points_gdf)\n",
    "        \n",
    "        # Xuất kết quả\n",
    "        output_prefix = f'group_{i+1}'\n",
    "        print(f\"Đang xuất kết quả cho nhóm dữ liệu {i+1}...\")\n",
    "        export_outputs(grid_interpolated, output_folder, output_prefix, grid.crs)\n",
    "    \n",
    "    print(\"Hoàn thành!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11c1651-f6d5-432a-a326-678b88633f1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
